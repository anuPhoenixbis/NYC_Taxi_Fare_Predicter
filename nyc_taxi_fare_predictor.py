# -*- coding: utf-8 -*-
"""NYC_Taxi_Fare_Predicter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zY1cBTsxLK-gn6JarHkQpXcelwvxw8b6

We will use the NYC taxi fare dataset to train a model and build a final better model to compete in the kaggle competition

its all ways a good practice to break down our final goal into sectional parts and sub-sections in those sections

Outline of our project:
1. Download the dataset
2. Explore the dataset
3. Cleaning of the dataset
4. Splitting the dataset for training
5. Train hardcoded/baseline models for comparison
6. Make predictions and submit to Kaggle
7. Perform feature Engineering
8. Train and evalute different models
9. Tune hyper-params of the selected model
10. Train with the entire dataset
11. Document and publish the project online

##Download the dataset

sub-tasks:
* install the required libs and import them
* download the dataset(s)
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy matplotlib seaborn scikit-learn plotly opendatasets --quiet

# Commented out IPython magic to ensure Python compatibility.
# %pip install xgboost --quiet

# Commented out IPython magic to ensure Python compatibility.
#basic imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import plotly.express as px
import opendatasets as od
import os
# %matplotlib inline

#graph initailizations
sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10, 6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

dataset_url = 'https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview'

od.download(dataset_url)

os.listdir('./new-york-city-taxi-fare-prediction')

"""##Exploration of the datasets

Our goal here:
* create a df of the files given to us
* the actual train set is pretty huge so, take a sample of the dataset for further evaluation
* do the required edits or combining of the datasets
* explore the dataset using the plots and draw conclusions from it

To optimize our datasets:
* ignore the key col
* Parse pickup datetime while loading data
* handle other cols like geo coords , fare amt , passenger cnt
"""

sample_frac =0.01

#removing the key col
selected_cols = 'fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count'.split(',')
selected_cols

#parsing of the datetime
dtypes = {
    'fare_amount': 'float32',
    'pickup_longitude': 'float32',
    'pickup_latitude': 'float32',
    'dropoff_longitude': 'float32',
    'dropoff_latitude' : 'float32',
    'passenger_count': 'uint8'
}

import random
random.seed(42)
#to take the required sample rows and skip the other rows
def skip_row(row_index):
    if row_index == 0:
        return False
    return random.random() > sample_frac

"""The expression random.random() > sample_frac compares the random number to sample_frac. If the random number is greater than sample_frac, the function returns True, indicating that the row should be skipped. Otherwise, it returns False, meaning the row should be included."""

df=pd.read_csv('./new-york-city-taxi-fare-prediction/train.csv',usecols=selected_cols,parse_dates=['pickup_datetime'],dtype=dtypes,skiprows=skip_row)

"""1. usecols=selected_cols: This argument specifies which columns from the CSV file should be included in the DataFrame.

2. parse_dates=['pickup_datetime']: This tells pandas to treat the 'pickup_datetime' column as dates and automatically convert them into a proper date format.

3. dtype=dtypes: This argument defines the data types for specific columns.

4. skiprows=skip_row: This argument allows for skipping specific rows while reading the data. It utilizes the skip_row function, which likely implements logic to randomly select a fraction of the data for analysis.
"""

test_df=pd.read_csv('./new-york-city-taxi-fare-prediction/test.csv')
test_df

df

df.info()

df.describe()

df['pickup_datetime'].min(),df['pickup_datetime'].max()

"""observations:
1. missing values
2. fare ranges (-52 to 499)
3. passenger_cnt(0 to 208)
5. definitely there are some outliers
"""

test_df.info()

test_df.describe()

test_df['pickup_datetime'].min(),test_df['pickup_datetime'].max()

"""the test_df and df pickup_datetime ranges are the same

##Cleaning of the dataset

lets first explore using the AIs only
"""

# from google.colab import files

# # Assuming your DataFrame is named 'df'
# df.to_csv('train_sample.csv', index=False)
# files.download('train_sample.csv')

sns.barplot(df,x='passenger_count',y='fare_amount')

"""there are some datapoints with 208 as passenger count which is implausible lets remove them"""

#keeping the data points with passenger count from 1-6
df=df[df['passenger_count'].between(1,6)]
sns.barplot(df,x='passenger_count',y='fare_amount')

"""lets drop the nan cols from both the datasets"""

df=df.dropna()
test_df=test_df.dropna()
test_df.info()

"""lets check if there are fares below 0"""

#checking if the no of fare below 0
cnt = df[df['fare_amount']<=0]['fare_amount'].value_counts()
cnt

"""fare can't be negative or 0 so lets remove those rows"""

df=df[df['fare_amount']>0]
df[df['fare_amount']<=0]['fare_amount'].value_counts().sum()

"""lets look at the coords now"""

px.scatter(df,x='pickup_longitude',y='pickup_latitude',color='fare_amount',range_color=[0,50])

"""lets eliminate the out of range outliers first then we will concentrate for the NYC"""

#keeping the lat and long within range
df=df[df['pickup_longitude'].between(-180,180)]
df=df[df['pickup_latitude'].between(-90,90)]
df=df[df['dropoff_longitude'].between(-180,180)]
df=df[df['dropoff_latitude'].between(-90,90)]
px.scatter(df,x='pickup_longitude',y='pickup_latitude',color='fare_amount',range_color=[0,50])

df=df[df['pickup_longitude'].between(-75, -73)]
df=df[df['pickup_latitude'].between(40, 42)]
df=df[df['dropoff_longitude'].between(-75, -73)]
df=df[df['dropoff_latitude'].between(40, 42)]
px.scatter(df,x='pickup_longitude',y='pickup_latitude',color='fare_amount',range_color=[0,50])

"""the coords look good now

lets handle the pickup_datetime as well
"""

df['pickup_datetime']

Date=pd.to_datetime(df['pickup_datetime'])
df['Day']=Date.dt.day
df['Month']=Date.dt.month
df['Year']=Date.dt.year
df['Hour']=Date.dt.hour
df=df.drop(columns='pickup_datetime')
df

"""here we are done with the cleaning of the dataset

##Prepare the dataset for training

here we will split the dataset

lets keep 20% for val set based on year
"""

from sklearn.model_selection import train_test_split

train_df,val_df = train_test_split(df,test_size=0.2,random_state=42)
len(train_df),len(val_df)

train_df.columns

#separating the input and target cols
input_cols=[ 'pickup_longitude', 'pickup_latitude',
       'dropoff_longitude', 'dropoff_latitude', 'passenger_count']
target_cols='fare_amount'

train_inputs=train_df[input_cols]
train_targets=train_df[target_cols]
val_inputs=val_df[input_cols]
val_targets=val_df[target_cols]

test_inputs=test_df[input_cols]
test_inputs

"""##Baseline Models Testing

MeanRegressor :  always returns the avg
"""

class MeanRegressor:
  def fit(self,inputs,targets):
    self.mean = targets.mean()
  def predict(self,inputs):
    return np.full(train_inputs.shape[0],self.mean) #train_inputs return a array of [rows_number,cols_number]
    #we are returning an array containing the means

mean_model = MeanRegressor()
mean_model.fit(train_inputs,train_targets)

mean_model.mean #return the mean of the values fit into it , the value returned is same as the starter model given in the kaggle overview page
#thus, it was mean model

train_preds = mean_model.predict(train_inputs)
train_preds
#it returns the mean value for all the predictions

val_preds = mean_model.predict(val_inputs)
val_preds

"""evaluating the rmse"""

from sklearn.metrics import mean_squared_error

def rmse(targets,preds):
  return np.sqrt(mean_squared_error(targets,preds))

rmse(train_targets,train_preds)

"""this means the values predicted are off by 9 which is quite bad as per the actual fare ranges from 7-11

LinearRegression
"""

from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
linreg.fit(train_inputs,train_targets)

train_preds = linreg.predict(train_inputs)
val_preds = linreg.predict(val_inputs)
train_preds

val_preds

linreg.score(train_inputs,train_targets) , linreg.score(val_inputs,val_targets)

#lets check the rmse
rmse(train_targets,train_preds),rmse(val_targets,val_preds)

"""these are also off by 9 so it no good than the mean_regressor

##Submit to kaggle

submit to kaggle as often as possible to track on how much better we are getting in it with our various tactics and models
"""

test_preds = linreg.predict(test_inputs)
test_preds

"""lets add these to the submission file"""

submission_df=pd.read_csv('./new-york-city-taxi-fare-prediction/sample_submission.csv')
submission_df

"""replace the values in the fare_amount col with our test_preds"""

submission_df['fare_amount']=test_preds
submission_df

def generate_submission(test_preds,filename):
  submission_df=pd.read_csv('./new-york-city-taxi-fare-prediction/sample_submission.csv')
  submission_df['fare_amount']=test_preds
  submission_df.to_csv(filename,index=False)

generate_submission(test_preds,'submission.csv')

"""Create some general funcs to ease the task that are repetatitve"""

def predict_and_submit(model,test_inputs,filename):
  test_preds = model.predict(test_inputs)
  generate_submission(test_preds,filename)
  return test_preds

predict_and_submit(linreg,test_inputs,'submission_linreg.csv')

"""##Feature Engineering

each day we might come across and try out various new ideas and its very useful to keep track of them and also keeping a note to what and how they helped or not helped in an excel sheet or maybe notion as per your wish

our goal here:
* extract date parts
* remove outliers and invalid data
* add distance between pick and drop
* add distance from landmarks

we have extracted most of the date parts and also dealt with some of the point 2 previously
"""

#we have to extract the weekday and hour as well
df['Weekday']=Date.dt.weekday
df['Hour']=Date.dt.hour
df

"""lets create a func for these as well for further use"""

def add_dateparts(df,col):
  Date=pd.to_datetime(df[col])
  df['_Day']=Date.dt.day
  df['Month']=Date.dt.month
  df['Year']=Date.dt.year
  df['WeekDay']=Date.dt.weekday
  df['Hour']=Date.dt.hour
  return df

test_df['pickup_datetime']=pd.to_datetime(test_df['pickup_datetime'])
add_dateparts(test_df,'pickup_datetime')
test_df

"""Now get the dist b/w 2 pts using lats and longs

We will use the haversine formula to do so

just search and get this code snippet from claude or gpt
"""

def haversine_np(lon1, lat1, lon2, lat2):
    """
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees)

    All args must be of equal length.

    """
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])

    dlon = lon2 - lon1
    dlat = lat2 - lat1

    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2

    c = 2 * np.arcsin(np.sqrt(a))
    km = 6367 * c
    return km

def add_trip_distance(df):
  df['trip_distance']=haversine_np(df['pickup_longitude'],df['pickup_latitude'],df['dropoff_longitude'],df['dropoff_latitude'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# add_trip_distance(train_df)
# add_trip_distance(val_df)
# add_trip_distance(test_df)

train_df

"""Popular Landmarks

adding in popular landmarks as it will very much affect the fare prices , near the landmarks the fare prices will be much higher than the usual

- JFK Airport
- LGA Airport
- EWR Airport
- Times Square
- Met Meuseum
- World Trade Center
- Statue of Liberty
- Empire State Building
- Central Park
- Brooklyn Bridge

We'll add the distance from drop location.
"""

#lets feed in the coords of these locations
jfk_lonlat = -73.7781, 40.6413
lga_lonlat = -73.8740, 40.7769
ewr_lonlat = -74.1745, 40.6895
met_lonlat = -73.9632, 40.7794
wtc_lonlat = -74.0099, 40.7126
sol_lonlat = -74.0445, 40.6892
esb_lonlat = -73.9857, 40.7488
cp_lonlat = -73.9683, 40.7851
bb_lonlat = -73.9969, 40.7061

def add_landmark_dropoff_distance(df, landmark_name, landmark_lonlat):
    lon, lat = landmark_lonlat #this contains the lats and longs of the current landmark
    df[landmark_name + '_drop_distance']=haversine_np(lon, lat, df['dropoff_longitude'], df['dropoff_latitude'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for a_df in [train_df, val_df, test_df]:
#     for name, lonlat in [('jfk', jfk_lonlat),
#                          ('lga', lga_lonlat),
#                          ('ewr', ewr_lonlat),
#                           ('met', met_lonlat),
#                           ('wtc', wtc_lonlat),
#                            ('sol', sol_lonlat),
#                           ('esb', esb_lonlat),
#                           ('cp', cp_lonlat),
#                           ('bb', bb_lonlat)]:
#         add_landmark_dropoff_distance(a_df, name, lonlat)

train_df

val_df

test_df

"""to deal with outliers lets add a helper func"""

# - `fare_amount`: \$1 to \$500
# - `longitudes`: -75 to -72
# - `latitudes`: 40 to 42
# - `passenger_count`: 1 to 6


def remove_outliers(df):
    return df[(df['fare_amount'] >= 1.) &
              (df['fare_amount'] <= 500.) &
              (df['pickup_longitude'] >= -75) &
              (df['pickup_longitude'] <= -72) &
              (df['dropoff_longitude'] >= -75) &
              (df['dropoff_longitude'] <= -72) &
              (df['pickup_latitude'] >= 40) &
              (df['pickup_latitude'] <= 42) &
              (df['dropoff_latitude'] >=40) &
              (df['dropoff_latitude'] <= 42) &
              (df['passenger_count'] >= 1) &
              (df['passenger_count'] <= 6)]

train_df=remove_outliers(train_df)
val_df=remove_outliers(val_df)
val_df

"""Scaling and One-Hot Encoding

we are gonna use the tree-based models which handles these perfectly

##Train and Evalute other models

We will train 3 models:
- ridge
- RF
- XGBoost
- Lasso

splitting the inputs and targets
"""

train_df.columns

input_cols=[ 'pickup_longitude', 'pickup_latitude',
       'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'Day',
       'Month', 'Year', 'Hour', 'trip_distance', 'jfk_drop_distance',
       'lga_drop_distance', 'ewr_drop_distance', 'met_drop_distance',
       'wtc_drop_distance', 'sol_drop_distance', 'esb_drop_distance',
       'cp_drop_distance', 'bb_drop_distance']

target_cols='fare_amount'

train_inputs=train_df[input_cols]
train_targets=train_df[target_cols]
val_inputs=val_df[input_cols]
val_targets=val_df[target_cols]

"""lets do it test_df as well"""

# Apply feature engineering to test_df
test_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'])
add_dateparts(test_df, 'pickup_datetime')

# Ensure 'Day' and 'Time' columns are present in test_df
test_df['Day'] = test_df['_Day']  # Assuming '_Day' was created by add_dateparts
test_df['Time'] = test_df['pickup_datetime'].dt.time

# Now you can safely select the input columns
test_inputs = test_df[input_cols]
test_inputs

"""evaluate the models func"""

def evaluate_models(model):
  train_preds = model.predict(train_inputs)
  val_preds = model.predict(val_inputs)
  train_rmse = rmse(train_targets,train_preds)
  val_rmse = rmse(val_targets,val_preds)
  return train_rmse,val_rmse,train_preds,val_preds

"""ridge regression"""

from sklearn.linear_model import Ridge

model_ridge = Ridge(alpha=0.9,random_state=42)
#ridge is similar to linearregression only catch is it adds penalty for wrong or far away values
#alpha is the penalty here

"""basically removing time and adding hour col"""

# Removing 'Time' column if it exists
if 'Time' in train_df.columns:
    train_df = train_df.drop(columns=['Time'])
if 'Time' in val_df.columns:
    val_df = val_df.drop(columns=['Time'])

model_ridge.fit(train_inputs,train_targets)

evaluate_models(model_ridge)

"""5 as train_rmse and 4.9 as val_rmse which is a drastic jump from the baseline models"""

#download the ridge submission
predict_and_submit(model_ridge,test_inputs,'submission_ridge.csv')

"""RF"""

from sklearn.ensemble import RandomForestRegressor

model_RF=RandomForestRegressor(n_estimators=100,random_state=42,n_jobs=-1,max_depth=10)

# %%time
# model_RF.fit(train_inputs,train_targets)

# evaluate_models(model_RF)

# predict_and_submit(model_RF,test_inputs,'submission_RF.csv')

"""RF takes a reallly long time so better move onto the next best one

xgboost
"""

from xgboost import XGBRegressor

model_xgb = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror').fit(train_inputs, train_targets)

evaluate_models(model_xgb)

predict_and_submit(model_xgb,test_inputs,'submission_xgb.csv')

"""we are just off by 3.9 in val set it again better than ridge and 3.5 on test set"""

from sklearn.linear_model import Lasso

model_lasso = Lasso(alpha=0.001,random_state=42).fit(train_inputs,train_targets)

evaluate_models(model_lasso)

predict_and_submit(model_lasso,test_inputs,'submission_lasso.csv')

"""this is worse than xgboost

XGBOOST was the best one so we will proceed with the hyper params tuning of xgboost only

##HyperParameters Tuning

- Tune the most important/impactful hyperparameter first e.g. n_estimators

- With the best value of the first hyperparameter, tune the next most impactful hyperparameter

- And so on, keep training the next most impactful parameters with the best values for previous parameters...

- Then, go back to the top and further tune each parameter again for further marginal gains

- Hyperparameter tuning is more art than science, unfortunately. Try to get a feel for how the parameters interact with each other based on your understanding of the parameterâ€¦

lets create some helper funcs to deal with it
"""

def test_params(ModelClass, **params):#tweak params to get the best result
    """Trains a model with the given parameters and returns training & validation RMSE"""
    model = ModelClass(**params).fit(train_inputs, train_targets)
    train_rmse = np.sqrt(mean_squared_error(model.predict(train_inputs), train_targets))
    val_rmse = np.sqrt(mean_squared_error(model.predict(val_inputs), val_targets))
    return train_rmse, val_rmse

def test_param_and_plot(ModelClass, param_name, param_values, **other_params): #keeping the param_values fixed tweak other_params
    """Trains multiple models by varying the value of param_name according to param_values"""
    train_errors, val_errors = [], []
    for value in param_values:
        params = dict(other_params)
        params[param_name] = value
        train_rmse, val_rmse = test_params(ModelClass, **params)
        train_errors.append(train_rmse)
        val_errors.append(val_rmse)

    plt.figure(figsize=(10,6))
    plt.title('Overfitting curve: ' + param_name)
    plt.plot(param_values, train_errors, 'b-o')
    plt.plot(param_values, val_errors, 'r-o')
    plt.xlabel(param_name)
    plt.ylabel('RMSE')
    plt.legend(['Training', 'Validation'])

best_params = {
    'random_state': 42,
    'n_jobs': -1,
    'objective': 'reg:squarederror'
}

"""tuning n_estimators"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# test_param_and_plot(XGBRegressor, 'n_estimators', [100, 250, 500], **best_params)

"""500 is the best but not much difference from 250 so go on with it

max_depth
"""

best_params['n_estimators'] = 250

test_param_and_plot(XGBRegressor, 'max_depth', [3, 5, 6], **best_params)

"""5 is best here

learning rates
"""

best_params['max_depth'] = 5

test_param_and_plot(XGBRegressor, 'learning_rate', [0.01, 0.05, 0.1], **best_params)

"""best will be .1"""

best_params['learning_rate']=.1

"""finally after much testing"""

model_xgb= XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42,
                               n_estimators=500, max_depth=5, learning_rate=0.1,
                               subsample=0.8, colsample_bytree=0.8)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model_xgb.fit(train_inputs,train_targets)

evaluate_models(model_xgb)

"""still better than the previous models"""

predict_and_submit(model_xgb,test_inputs,'submission_xgb_tuned.csv')

"""##exporting the model"""

import pickle

# Save your trained model to a pickle file
with open('model.pkl', 'wb') as f:
    pickle.dump(model_xgb, f)

# Download the file to your local computer
from google.colab import files
files.download('model.pkl')

input_cols

import xgboost as xgb

# Assuming `model` is your trained XGBoost model
model_xgb.save_model('model.json')